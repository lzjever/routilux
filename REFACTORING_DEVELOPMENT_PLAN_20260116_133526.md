# çº¿ç¨‹æ¨¡å‹é‡æ„å¼€å‘è®¡åˆ’

## æ‰§è¡Œæ‘˜è¦

æœ¬è®¡åˆ’è¯¦ç»†æè¿°å°†Routiluxä»Flowçº§åˆ«çš„æ‰§è¡ŒçŠ¶æ€è¿ç§»åˆ°Jobçº§åˆ«çš„æ‰§è¡Œä¸Šä¸‹æ–‡çš„å®Œæ•´é‡æ„è¿‡ç¨‹ã€‚è¿™æ˜¯ä¸€ä¸ª**é‡å¤§æ¶æ„å˜æ›´**ï¼Œéœ€è¦ä¸¥æ ¼æŒ‰ç…§æœ¬è®¡åˆ’æ‰§è¡Œï¼Œ**ä¸å…è®¸ä»»ä½•è‡ªç”±å‘æŒ¥**ã€‚

## è®¾è®¡å®¡æŸ¥ç»“æœ

### âœ… è®¾è®¡æ–¹æ¡ˆè¯„ä¼°

ç»è¿‡ä¸¥æ ¼å®¡æŸ¥ï¼Œè®¾è®¡æ–¹æ¡ˆ**ç¬¦åˆæœ€ä½³å®è·µ**ï¼Œä½†éœ€è¦è¡¥å……ä»¥ä¸‹å…³é”®ç‚¹ï¼š

1. âœ… **æ¶æ„è®¾è®¡æ­£ç¡®**ï¼šJobçº§åˆ«éš”ç¦»ã€å…¨å±€çº¿ç¨‹æ± ã€åºåˆ—åŒ–å‹å¥½
2. âš ï¸ **éœ€è¦è¡¥å……**ï¼šemit()è·¯ç”±æœºåˆ¶ã€æš‚åœ/æ¢å¤/å–æ¶ˆã€è¶…æ—¶å¤„ç†ã€é”™è¯¯æ¸…ç†
3. âš ï¸ **éœ€è¦æ˜ç¡®**ï¼šFlow.execute()çš„å¤„ç†æ–¹å¼ï¼ˆç”¨æˆ·è¯´ä¸éœ€è¦å‘åå…¼å®¹ï¼‰

### ğŸ”´ å…³é”®é—æ¼ç‚¹

1. **emit()è·¯ç”±é—®é¢˜**ï¼šå½“å‰`emit()`é€šè¿‡`flow._enqueue_task()`ï¼Œä½†æ–°è®¾è®¡ä¸­Flowæ²¡æœ‰task_queueï¼Œéœ€è¦è·¯ç”±åˆ°æ­£ç¡®çš„JobExecutor
2. **æš‚åœ/æ¢å¤/å–æ¶ˆ**ï¼šéœ€è¦å®ç°è¿™äº›åŠŸèƒ½åœ¨JobExecutorä¸­
3. **Monitoring hooks**ï¼šéœ€è¦ç¡®ä¿åœ¨JobExecutorä¸­æ­£ç¡®è°ƒç”¨
4. **è¶…æ—¶å¤„ç†**ï¼šéœ€è¦å®ç°è¶…æ—¶æœºåˆ¶
5. **JobExecutoræ¸…ç†**ï¼šjobå®Œæˆåéœ€è¦ä»GlobalJobManagerä¸­ç§»é™¤
6. **é”™è¯¯å¤„ç†**ï¼šéœ€è¦ç¡®ä¿é”™è¯¯æ—¶æ­£ç¡®æ¸…ç†èµ„æº

## å¼€å‘è®¡åˆ’

### Phase 1: åˆ›å»ºæ ¸å¿ƒåŸºç¡€è®¾æ–½

#### Step 1.1: åˆ›å»ºGlobalJobManageræ¨¡å—

**æ–‡ä»¶**: `routilux/job_manager.py` (æ–°å»º)

**ä»»åŠ¡**:
1. åˆ›å»º`GlobalJobManager`ç±»ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
2. å®ç°çº¿ç¨‹å®‰å…¨çš„å•ä¾‹åˆå§‹åŒ–
3. å®ç°å…¨å±€çº¿ç¨‹æ± ç®¡ç†
4. å®ç°jobæ³¨å†Œå’ŒæŸ¥è¯¢

**å…·ä½“å®ç°è¦æ±‚**:

```python
"""
Global job manager for managing all job executions.

This module provides a singleton GlobalJobManager that manages:
- Global thread pool (shared by all jobs)
- Running job registry
- Job lifecycle management
"""

import logging
import threading
from concurrent.futures import ThreadPoolExecutor
from typing import TYPE_CHECKING, Optional

if TYPE_CHECKING:
    from routilux.flow.flow import Flow
    from routilux.job_state import JobState

logger = logging.getLogger(__name__)

# Global instance
_global_job_manager: Optional["GlobalJobManager"] = None
_global_job_manager_lock = threading.Lock()


class GlobalJobManager:
    """Global job manager (singleton).
    
    Manages all job executions with a shared thread pool.
    """
    
    def __init__(self, max_workers: int = 100):
        """Initialize global job manager.
        
        Args:
            max_workers: Maximum number of worker threads in global thread pool.
        """
        self.max_workers = max_workers
        self.global_thread_pool = ThreadPoolExecutor(
            max_workers=max_workers,
            thread_name_prefix="RoutiluxWorker"
        )
        self.running_jobs: dict[str, "JobExecutor"] = {}
        self._lock = threading.Lock()
        self._shutdown = False
    
    def start_job(
        self,
        flow: "Flow",
        entry_routine_id: str,
        entry_params: dict | None = None,
        timeout: float | None = None,
        job_state: "JobState" | None = None,
    ) -> "JobState":
        """Start a new job execution.
        
        Args:
            flow: Flow to execute.
            entry_routine_id: Entry routine identifier.
            entry_params: Entry parameters.
            timeout: Execution timeout in seconds.
            job_state: Optional existing JobState to use.
        
        Returns:
            JobState object (status will be RUNNING).
        
        Raises:
            RuntimeError: If manager is shut down.
            ValueError: If entry_routine_id not found.
        """
        if self._shutdown:
            raise RuntimeError("GlobalJobManager is shut down")
        
        if entry_routine_id not in flow.routines:
            raise ValueError(f"Entry routine '{entry_routine_id}' not found in flow")
        
        from routilux.job_state import JobState
        from routilux.status import ExecutionStatus
        
        # Create or use provided JobState
        if job_state is None:
            job_state = JobState(flow.flow_id)
        
        job_state.status = ExecutionStatus.RUNNING
        job_state.current_routine_id = entry_routine_id
        
        # Create JobExecutor
        from routilux.job_executor import JobExecutor
        
        executor = JobExecutor(
            flow=flow,
            job_state=job_state,
            global_thread_pool=self.global_thread_pool,
            timeout=timeout,
        )
        
        # Register job
        with self._lock:
            if self._shutdown:
                raise RuntimeError("GlobalJobManager is shut down")
            self.running_jobs[job_state.job_id] = executor
        
        # Start execution
        executor.start(entry_routine_id, entry_params or {})
        
        return job_state
    
    def get_job(self, job_id: str) -> Optional["JobExecutor"]:
        """Get job executor by job_id.
        
        Args:
            job_id: Job identifier.
        
        Returns:
            JobExecutor if found, None otherwise.
        """
        with self._lock:
            return self.running_jobs.get(job_id)
    
    def wait_for_all_jobs(self, timeout: float | None = None) -> bool:
        """Wait for all running jobs to complete.
        
        Args:
            timeout: Maximum time to wait in seconds. None for infinite wait.
        
        Returns:
            True if all jobs completed, False if timeout.
        """
        import time
        
        start_time = time.time()
        
        while True:
            # Check timeout
            if timeout is not None:
                elapsed = time.time() - start_time
                if elapsed >= timeout:
                    return False
            
            # Check if all jobs are done
            with self._lock:
                running = [
                    job_id for job_id, executor in self.running_jobs.items()
                    if executor.is_running()
                ]
            
            if not running:
                return True
            
            time.sleep(0.1)
    
    def shutdown(self, wait: bool = True, timeout: float | None = None):
        """Shutdown global job manager.
        
        Args:
            wait: Whether to wait for jobs to complete.
            timeout: Wait timeout in seconds.
        """
        with self._lock:
            if self._shutdown:
                return
            
            self._shutdown = True
            
            # Stop all running jobs
            for executor in list(self.running_jobs.values()):
                executor.stop()
            
            self.running_jobs.clear()
        
        # Shutdown thread pool
        if wait:
            self.global_thread_pool.shutdown(wait=True, timeout=timeout)
        else:
            self.global_thread_pool.shutdown(wait=False)


def get_job_manager(max_workers: int = 100) -> GlobalJobManager:
    """Get or create global job manager instance.
    
    Args:
        max_workers: Maximum workers (only used on first call).
    
    Returns:
        GlobalJobManager instance.
    """
    global _global_job_manager
    
    if _global_job_manager is None:
        with _global_job_manager_lock:
            if _global_job_manager is None:
                _global_job_manager = GlobalJobManager(max_workers=max_workers)
    
    return _global_job_manager
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] å•ä¾‹æ¨¡å¼çº¿ç¨‹å®‰å…¨
- [ ] å…¨å±€çº¿ç¨‹æ± æ­£ç¡®åˆ›å»º
- [ ] jobæ³¨å†Œå’ŒæŸ¥è¯¢åŠŸèƒ½æ­£å¸¸
- [ ] shutdownåŠŸèƒ½æ­£ç¡®

---

#### Step 1.2: åˆ›å»ºJobExecutoræ¨¡å—

**æ–‡ä»¶**: `routilux/job_executor.py` (æ–°å»º)

**ä»»åŠ¡**:
1. åˆ›å»º`JobExecutor`ç±»
2. å®ç°ç‹¬ç«‹çš„task queueå’Œevent loop
3. å®ç°ä»»åŠ¡æ‰§è¡Œé€»è¾‘
4. å®ç°æš‚åœ/æ¢å¤/å–æ¶ˆåŠŸèƒ½
5. å®ç°è¶…æ—¶å¤„ç†
6. å®ç°å®Œæˆæ£€æµ‹å’Œæ¸…ç†

**å…·ä½“å®ç°è¦æ±‚**:

```python
"""
Job executor for managing individual job execution context.

Each job has its own JobExecutor instance with:
- Independent task queue
- Independent event loop thread
- Reference to global thread pool
"""

import logging
import queue
import threading
import time
from datetime import datetime
from typing import TYPE_CHECKING, Optional

if TYPE_CHECKING:
    from concurrent.futures import ThreadPoolExecutor
    from routilux.flow.flow import Flow
    from routilux.flow.task import SlotActivationTask
    from routilux.job_state import JobState

logger = logging.getLogger(__name__)


class JobExecutor:
    """Manages execution context for a single job."""
    
    def __init__(
        self,
        flow: "Flow",
        job_state: "JobState",
        global_thread_pool: "ThreadPoolExecutor",
        timeout: float | None = None,
    ):
        """Initialize job executor.
        
        Args:
            flow: Flow to execute.
            job_state: JobState for this execution.
            global_thread_pool: Global thread pool to use.
            timeout: Execution timeout in seconds.
        """
        self.flow = flow
        self.job_state = job_state
        self.global_thread_pool = global_thread_pool
        self.timeout = timeout
        
        # Independent execution context
        self.task_queue = queue.Queue()
        self.pending_tasks: list["SlotActivationTask"] = []
        self.event_loop_thread: Optional[threading.Thread] = None
        self.active_tasks: set = set()
        self._running = False
        self._paused = False
        self._lock = threading.Lock()
        self._start_time: Optional[float] = None
        
        # Execution tracker (one per job)
        from routilux.execution_tracker import ExecutionTracker
        self.execution_tracker: Optional[ExecutionTracker] = None
        
        # Set flow context for routines
        for routine in flow.routines.values():
            routine._current_flow = flow
    
    def start(self, entry_routine_id: str, entry_params: dict):
        """Start job execution.
        
        Args:
            entry_routine_id: Entry routine identifier.
            entry_params: Entry parameters.
        
        Raises:
            ValueError: If entry routine not found or no trigger slot.
        """
        from routilux.status import ExecutionStatus
        
        if entry_routine_id not in self.flow.routines:
            raise ValueError(f"Entry routine '{entry_routine_id}' not found")
        
        entry_routine = self.flow.routines[entry_routine_id]
        trigger_slot = entry_routine.get_slot("trigger")
        
        if trigger_slot is None:
            raise ValueError(
                f"Entry routine '{entry_routine_id}' must have 'trigger' slot"
            )
        
        # Update job state
        self.job_state.status = ExecutionStatus.RUNNING
        self.job_state.current_routine_id = entry_routine_id
        self._start_time = time.time()
        
        # Record execution start
        self.job_state.record_execution(entry_routine_id, "start", entry_params)
        
        # Create execution tracker (one per job)
        from routilux.execution_tracker import ExecutionTracker
        self.execution_tracker = ExecutionTracker(self.flow.flow_id)
        self.execution_tracker.record_routine_start(entry_routine_id, entry_params)
        
        # Monitoring hook
        from routilux.monitoring.hooks import execution_hooks
        execution_hooks.on_flow_start(self.flow, self.job_state)
        
        # Start event loop
        self._running = True
        self.event_loop_thread = threading.Thread(
            target=self._event_loop,
            daemon=True,
            name=f"JobExecutor-{self.job_state.job_id[:8]}"
        )
        self.event_loop_thread.start()
        
        # Trigger entry routine
        self._trigger_entry(entry_routine_id, entry_params)
    
    def _event_loop(self):
        """Event loop main logic."""
        while self._running:
            try:
                # Check timeout
                if self.timeout is not None and self._start_time is not None:
                    elapsed = time.time() - self._start_time
                    if elapsed >= self.timeout:
                        logger.warning(
                            f"Job {self.job_state.job_id} timed out after {self.timeout}s"
                        )
                        self._handle_timeout()
                        break
                
                # Check if paused
                if self._paused:
                    time.sleep(0.01)
                    continue
                
                # Get task from queue
                try:
                    task = self.task_queue.get(timeout=0.1)
                except queue.Empty:
                    # Check if complete
                    if self._is_complete():
                        self._handle_completion()
                        break
                    continue
                
                # Submit to global thread pool
                future = self.global_thread_pool.submit(
                    self._execute_task, task
                )
                
                with self._lock:
                    self.active_tasks.add(future)
                
                def on_done(fut=future):
                    with self._lock:
                        self.active_tasks.discard(fut)
                    self.task_queue.task_done()
                
                future.add_done_callback(on_done)
                
            except Exception as e:
                logger.exception(f"Error in event loop for job {self.job_state.job_id}: {e}")
                self._handle_error(e)
                break
        
        # Cleanup
        self._cleanup()
    
    def _execute_task(self, task: "SlotActivationTask"):
        """Execute a single task.
        
        Args:
            task: SlotActivationTask to execute.
        """
        from routilux.routine import _current_job_state
        
        # Set job_state in context
        old_job_state = _current_job_state.get(None)
        _current_job_state.set(self.job_state)
        
        try:
            # Apply parameter mapping if connection exists
            if task.connection:
                mapped_data = task.connection._apply_mapping(task.data)
            else:
                mapped_data = task.data
            
            # Set routine._current_flow for slot.receive()
            if task.slot.routine:
                task.slot.routine._current_flow = self.flow
            
            # Execute slot handler
            # Note: slot.receive() internally calls monitoring hooks
            # (on_routine_start, on_slot_call, on_routine_end)
            # So we don't need to call them here
            task.slot.receive(
                mapped_data,
                job_state=self.job_state,
                flow=self.flow
            )
            
        except Exception as e:
            from routilux.flow.error_handling import handle_task_error
            handle_task_error(task, e, self.flow)
        finally:
            # Restore context
            if old_job_state is not None:
                _current_job_state.set(old_job_state)
            else:
                _current_job_state.set(None)
    
    def _trigger_entry(self, entry_routine_id: str, entry_params: dict):
        """Trigger entry routine.
        
        Args:
            entry_routine_id: Entry routine identifier.
            entry_params: Entry parameters.
        """
        entry_routine = self.flow.routines[entry_routine_id]
        trigger_slot = entry_routine.get_slot("trigger")
        
        from routilux.flow.task import SlotActivationTask
        task = SlotActivationTask(
            slot=trigger_slot,
            data=entry_params,
            job_state=self.job_state,
            connection=None,
        )
        
        self.enqueue_task(task)
    
    def enqueue_task(self, task: "SlotActivationTask"):
        """Enqueue a task for execution.
        
        Args:
            task: SlotActivationTask to enqueue.
        """
        if self._paused:
            self.pending_tasks.append(task)
        else:
            self.task_queue.put(task)
    
    def _is_complete(self) -> bool:
        """Check if job is complete.
        
        Returns:
            True if queue is empty and no active tasks.
        """
        if not self.task_queue.empty():
            return False
        
        with self._lock:
            active = [f for f in self.active_tasks if not f.done()]
            return len(active) == 0
    
    def _handle_completion(self):
        """Handle job completion."""
        from routilux.status import ExecutionStatus
        from routilux.monitoring.hooks import execution_hooks
        
        if self.job_state.status != ExecutionStatus.FAILED:
            self.job_state.status = ExecutionStatus.COMPLETED
            
            # Record execution end
            if self.execution_tracker:
                entry_routine_id = self.job_state.current_routine_id
                if entry_routine_id:
                    self.execution_tracker.record_routine_end(entry_routine_id, "completed")
            
            execution_hooks.on_flow_end(self.flow, self.job_state, "completed")
    
    def _handle_timeout(self):
        """Handle job timeout."""
        from routilux.status import ExecutionStatus
        from routilux.monitoring.hooks import execution_hooks
        
        self.job_state.status = ExecutionStatus.FAILED
        self.job_state.shared_data["error"] = f"Job timed out after {self.timeout}s"
        execution_hooks.on_flow_end(self.flow, self.job_state, "failed")
    
    def _handle_error(self, error: Exception):
        """Handle job error.
        
        Args:
            error: Exception that occurred.
        """
        from routilux.status import ExecutionStatus
        from routilux.monitoring.hooks import execution_hooks
        
        self.job_state.status = ExecutionStatus.FAILED
        if "error" not in self.job_state.shared_data:
            self.job_state.shared_data["error"] = str(error)
        
        # Record execution end
        if self.execution_tracker:
            entry_routine_id = self.job_state.current_routine_id
            if entry_routine_id:
                self.execution_tracker.record_routine_end(entry_routine_id, "failed", error=str(error))
        
        execution_hooks.on_flow_end(self.flow, self.job_state, "failed")
    
    def _cleanup(self):
        """Cleanup job executor."""
        self._running = False
        
        # Remove from global job manager
        from routilux.job_manager import get_job_manager
        job_manager = get_job_manager()
        with job_manager._lock:
            job_manager.running_jobs.pop(self.job_state.job_id, None)
    
    def pause(self, reason: str = "", checkpoint: dict | None = None):
        """Pause job execution.
        
        Args:
            reason: Reason for pausing.
            checkpoint: Optional checkpoint data.
        """
        from routilux.flow.state_management import pause_job_executor
        pause_job_executor(self, reason, checkpoint)
    
    def resume(self) -> "JobState":
        """Resume job execution.
        
        Returns:
            Updated JobState.
        """
        from routilux.flow.state_management import resume_job_executor
        return resume_job_executor(self)
    
    def cancel(self, reason: str = ""):
        """Cancel job execution.
        
        Args:
            reason: Reason for cancellation.
        """
        from routilux.flow.state_management import cancel_job_executor
        cancel_job_executor(self, reason)
    
    def is_running(self) -> bool:
        """Check if job is running.
        
        Returns:
            True if running, False otherwise.
        """
        return self._running and not self._is_complete()
    
    def stop(self):
        """Stop job execution."""
        self._running = False
        if self.event_loop_thread:
            self.event_loop_thread.join(timeout=1.0)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ç‹¬ç«‹çš„task queueå’Œevent loop
- [ ] ä»»åŠ¡æ­£ç¡®æ‰§è¡Œ
- [ ] è¶…æ—¶å¤„ç†æ­£ç¡®
- [ ] å®Œæˆæ£€æµ‹æ­£ç¡®
- [ ] æ¸…ç†é€»è¾‘æ­£ç¡®

---

### Phase 2: ä¿®æ”¹emit()è·¯ç”±æœºåˆ¶

#### Step 2.1: ä¿®æ”¹Event.emit()æ–¹æ³•

**æ–‡ä»¶**: `routilux/event.py`

**ä»»åŠ¡**:
1. ä¿®æ”¹`emit()`æ–¹æ³•ï¼Œä½¿å…¶è·¯ç”±åˆ°æ­£ç¡®çš„JobExecutor
2. é€šè¿‡JobStateæ‰¾åˆ°å¯¹åº”çš„JobExecutor
3. å¦‚æœæ‰¾ä¸åˆ°JobExecutorï¼Œä½¿ç”¨legacyæ¨¡å¼ï¼ˆç›´æ¥è°ƒç”¨ï¼‰

**å…·ä½“å®ç°è¦æ±‚**:

```python
def emit(self, flow: "Flow" | None = None, **kwargs) -> None:
    """Emit the event and send data to all connected slots.
    
    Modified to route tasks to JobExecutor instead of Flow.
    """
    # Auto-detect flow from routine context if not provided
    if flow is None and self.routine:
        flow = getattr(self.routine, "_current_flow", None)
    
    # Get job_state from context
    from routilux.routine import _current_job_state
    job_state = _current_job_state.get(None)
    
    # If we have job_state, route to JobExecutor
    if job_state is not None and flow is not None:
        from routilux.job_manager import get_job_manager
        job_manager = get_job_manager()
        executor = job_manager.get_job(job_state.job_id)
        
        if executor is not None:
            # Route to JobExecutor
            for slot in self.connected_slots:
                connection = flow._find_connection(self, slot)
                from routilux.flow.task import SlotActivationTask
                task = SlotActivationTask(
                    slot=slot,
                    data=kwargs,
                    job_state=job_state,
                    connection=connection,
                )
                executor.enqueue_task(task)
            return
    
    # Legacy mode: direct call (no flow or no job_state)
    if flow is None:
        for slot in self.connected_slots:
            slot.receive(**kwargs)
        return
    
    # Fallback: use Flow's enqueue (for backward compatibility during transition)
    # This should not happen in new architecture
    logger.warning(
        f"emit() called with flow but no job_state context. "
        f"Using legacy mode. Event: {self.name}"
    )
    for slot in self.connected_slots:
        connection = flow._find_connection(self, slot)
        from routilux.flow.task import SlotActivationTask
        task = SlotActivationTask(
            slot=slot,
            data=kwargs,
            job_state=None,  # No job_state in legacy mode
            connection=connection,
        )
        flow._enqueue_task(task)  # Legacy method
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] emit()æ­£ç¡®è·¯ç”±åˆ°JobExecutor
- [ ] å¦‚æœæ²¡æœ‰job_stateï¼Œä½¿ç”¨legacyæ¨¡å¼
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡

---

#### Step 2.2: ç§»é™¤Flow._enqueue_task()æ–¹æ³•ï¼ˆæˆ–æ ‡è®°ä¸ºdeprecatedï¼‰

**æ–‡ä»¶**: `routilux/flow/flow.py`

**ä»»åŠ¡**:
1. ç§»é™¤`_enqueue_task()`æ–¹æ³•ï¼ˆå› ä¸ºä¸å†éœ€è¦ï¼‰
2. æˆ–è€…æ ‡è®°ä¸ºdeprecatedå¹¶ä¿ç•™ç”¨äºlegacyæ¨¡å¼

**å…·ä½“å®ç°è¦æ±‚**:

```python
# ç§»é™¤æˆ–æ ‡è®°ä¸ºdeprecated
def _enqueue_task(self, task: SlotActivationTask) -> None:
    """Enqueue task (DEPRECATED - use JobExecutor.enqueue_task instead).
    
    This method is kept for legacy compatibility only.
    New code should use JobExecutor.enqueue_task().
    """
    import warnings
    warnings.warn(
        "Flow._enqueue_task() is deprecated. "
        "Use JobExecutor.enqueue_task() instead.",
        DeprecationWarning,
        stacklevel=2
    )
    
    # Legacy implementation - should not be used in new code
    from routilux.flow.event_loop import enqueue_task
    enqueue_task(task, self)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ–¹æ³•æ ‡è®°ä¸ºdeprecated
- [ ] ä¸å½±å“ç°æœ‰legacyä»£ç 

---

### Phase 3: å®ç°æš‚åœ/æ¢å¤/å–æ¶ˆåŠŸèƒ½

#### Step 3.1: åˆ›å»ºJobExecutorçŠ¶æ€ç®¡ç†æ¨¡å—

**æ–‡ä»¶**: `routilux/flow/job_state_management.py` (æ–°å»º)

**ä»»åŠ¡**:
1. å®ç°`pause_job_executor()`å‡½æ•°
2. å®ç°`resume_job_executor()`å‡½æ•°
3. å®ç°`cancel_job_executor()`å‡½æ•°
4. å®ç°ä»»åŠ¡åºåˆ—åŒ–/ååºåˆ—åŒ–

**å…·ä½“å®ç°è¦æ±‚**:

```python
"""
State management for JobExecutor (pause, resume, cancel).

This module handles job-level state management, replacing
Flow-level state management.
"""

import logging
import queue
import time
from datetime import datetime
from typing import TYPE_CHECKING, Any, Dict, Optional

if TYPE_CHECKING:
    from routilux.job_executor import JobExecutor
    from routilux.job_state import JobState

logger = logging.getLogger(__name__)


def pause_job_executor(
    executor: "JobExecutor",
    reason: str = "",
    checkpoint: Optional[Dict[str, Any]] = None,
) -> None:
    """Pause job execution.
    
    Args:
        executor: JobExecutor to pause.
        reason: Reason for pausing.
        checkpoint: Optional checkpoint data.
    """
    executor._paused = True
    
    # Wait for active tasks
    _wait_for_active_tasks(executor)
    
    # Drain task queue
    max_wait = 2.0
    start_time = time.time()
    while not executor.task_queue.empty():
        if time.time() - start_time > max_wait:
            logger.warning(
                f"pause_job_executor: Timeout draining task queue. "
                f"Queue size: {executor.task_queue.qsize()}"
            )
            break
        try:
            task = executor.task_queue.get(timeout=0.1)
            executor.pending_tasks.append(task)
        except queue.Empty:
            break
    
    # Record pause point
    pause_point = {
        "timestamp": datetime.now().isoformat(),
        "reason": reason,
        "checkpoint": checkpoint or {},
        "pending_tasks_count": len(executor.pending_tasks),
        "active_tasks_count": len(executor.active_tasks),
        "queue_size": executor.task_queue.qsize(),
    }
    
    executor.job_state.pause_points.append(pause_point)
    executor.job_state._set_paused(reason=reason, checkpoint=checkpoint)
    
    # Serialize pending tasks
    _serialize_pending_tasks(executor)


def resume_job_executor(executor: "JobExecutor") -> "JobState":
    """Resume job execution.
    
    Args:
        executor: JobExecutor to resume.
    
    Returns:
        Updated JobState.
    """
    if executor.job_state.flow_id != executor.flow.flow_id:
        raise ValueError(
            f"JobState flow_id '{executor.job_state.flow_id}' "
            f"does not match Flow flow_id '{executor.flow.flow_id}'"
        )
    
    executor.job_state._set_running()
    executor._paused = False
    
    # Deserialize pending tasks
    _deserialize_pending_tasks(executor)
    
    # Restart event loop if needed
    if not executor._running or not executor.event_loop_thread.is_alive():
        executor._running = True
        executor.event_loop_thread = threading.Thread(
            target=executor._event_loop,
            daemon=True,
            name=f"JobExecutor-{executor.job_state.job_id[:8]}"
        )
        executor.event_loop_thread.start()
    
    return executor.job_state


def cancel_job_executor(executor: "JobExecutor", reason: str = "") -> None:
    """Cancel job execution.
    
    Args:
        executor: JobExecutor to cancel.
        reason: Reason for cancellation.
    """
    executor._running = False
    executor._paused = False
    
    # Cancel active tasks
    with executor._lock:
        for future in list(executor.active_tasks):
            if not future.done():
                future.cancel()
        executor.active_tasks.clear()
    
    executor.job_state._set_cancelled(reason=reason)


def _wait_for_active_tasks(executor: "JobExecutor") -> None:
    """Wait for all active tasks to complete."""
    check_interval = 0.05
    max_wait_time = 5.0
    start_time = time.time()
    
    while True:
        with executor._lock:
            active = [f for f in executor.active_tasks if not f.done()]
            if not active:
                break
        
        elapsed = time.time() - start_time
        if elapsed > max_wait_time:
            logger.warning(
                f"wait_for_active_tasks timed out after {max_wait_time}s. "
                f"Active tasks: {len(active)}"
            )
            break
        
        time.sleep(check_interval)


def _serialize_pending_tasks(executor: "JobExecutor") -> None:
    """Serialize pending tasks to JobState.
    
    Args:
        executor: JobExecutor to serialize tasks from.
    """
    serialized_tasks = []
    for task in executor.pending_tasks:
        # Find routine_id in flow (not routine._id)
        routine_id = None
        if task.slot.routine:
            for rid, r in executor.flow.routines.items():
                if r is task.slot.routine:
                    routine_id = rid
                    break
        
        connection = task.connection
        serialized = {
            "routine_id": routine_id,  # Flow's routine_id
            "slot_name": task.slot.name,
            "data": task.data,
            "connection_source_routine_id": (
                flow._get_routine_id(connection.source_event.routine)
                if connection and connection.source_event and connection.source_event.routine
                else None
            ),
            "connection_source_event_name": (
                connection.source_event.name if connection and connection.source_event else None
            ),
            "connection_target_routine_id": (
                flow._get_routine_id(connection.target_slot.routine)
                if connection and connection.target_slot and connection.target_slot.routine
                else None
            ),
            "connection_target_slot_name": (
                connection.target_slot.name if connection and connection.target_slot else None
            ),
            "param_mapping": connection.param_mapping if connection else {},
            "priority": task.priority.value,
            "retry_count": task.retry_count,
            "max_retries": task.max_retries,
            "created_at": task.created_at.isoformat() if task.created_at else None,
        }
        serialized_tasks.append(serialized)
    
    executor.job_state.pending_tasks = serialized_tasks


def _deserialize_pending_tasks(executor: "JobExecutor") -> None:
    """Deserialize pending tasks from JobState.
    
    Args:
        executor: JobExecutor to deserialize tasks to.
    """
    if not executor.job_state.pending_tasks:
        return
    
    from routilux.flow.task import SlotActivationTask, TaskPriority
    from datetime import datetime
    
    executor.pending_tasks = []
    for serialized in executor.job_state.pending_tasks:
        routine_id = serialized.get("routine_id")
        slot_name = serialized.get("slot_name")
        
        if not routine_id or routine_id not in executor.flow.routines:
            continue
        
        routine = executor.flow.routines[routine_id]
        slot = routine.get_slot(slot_name)
        if not slot:
            continue
        
        # Reconstruct connection
        connection = None
        if serialized.get("connection_source_routine_id"):
            source_routine_id = serialized.get("connection_source_routine_id")
            source_event_name = serialized.get("connection_source_event_name")
            target_routine_id = serialized.get("connection_target_routine_id")
            target_slot_name = serialized.get("connection_target_slot_name")
            
            if (source_routine_id in executor.flow.routines and 
                target_routine_id in executor.flow.routines):
                source_routine = executor.flow.routines[source_routine_id]
                target_routine = executor.flow.routines[target_routine_id]
                source_event = (
                    source_routine.get_event(source_event_name) if source_event_name else None
                )
                target_slot = (
                    target_routine.get_slot(target_slot_name) if target_slot_name else None
                )
                
                if source_event and target_slot:
                    connection = executor.flow._find_connection(source_event, target_slot)
        
        task = SlotActivationTask(
            slot=slot,
            data=serialized.get("data", {}),
            connection=connection,
            priority=TaskPriority(serialized.get("priority", TaskPriority.NORMAL.value)),
            retry_count=serialized.get("retry_count", 0),
            max_retries=serialized.get("max_retries", 0),
            created_at=(
                datetime.fromisoformat(serialized["created_at"])
                if serialized.get("created_at")
                else None
            ),
            job_state=executor.job_state,
        )
        
        executor.pending_tasks.append(task)
        
        # Enqueue task
        executor.enqueue_task(task)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æš‚åœåŠŸèƒ½æ­£ç¡®
- [ ] æ¢å¤åŠŸèƒ½æ­£ç¡®
- [ ] å–æ¶ˆåŠŸèƒ½æ­£ç¡®
- [ ] ä»»åŠ¡åºåˆ—åŒ–/ååºåˆ—åŒ–æ­£ç¡®

---

### Phase 4: ä¿®æ”¹Flowç±»

#### Step 4.1: ç§»é™¤Flowä¸­çš„æ‰§è¡ŒçŠ¶æ€å­—æ®µ

**æ–‡ä»¶**: `routilux/flow/flow.py`

**ä»»åŠ¡**:
1. ä»`__init__()`ä¸­ç§»é™¤æ‰§è¡ŒçŠ¶æ€å­—æ®µ
2. ä»`add_serializable_fields()`ä¸­ç§»é™¤æ‰§è¡ŒçŠ¶æ€å­—æ®µ
3. ç§»é™¤`_get_executor()`æ–¹æ³•
4. ç§»é™¤`set_execution_strategy()`ä¸­çš„executoråˆ›å»ºé€»è¾‘

**å…·ä½“å®ç°è¦æ±‚**:

```python
def __init__(
    self,
    flow_id: str | None = None,
    execution_strategy: str = "sequential",
    max_workers: int = 5,
    execution_timeout: float | None = None,
):
    """Initialize Flow.
    
    Args:
        flow_id: Flow identifier.
        execution_strategy: Execution strategy ("sequential" or "concurrent").
        max_workers: Max workers (for configuration only, not used for thread pool).
        execution_timeout: Default execution timeout.
    """
    super().__init__()
    self.flow_id: str = flow_id or str(uuid.uuid4())
    self.routines: dict[str, Routine] = {}
    self.connections: list[Connection] = []
    self.execution_tracker: ExecutionTracker | None = None
    self.error_handler: ErrorHandler | None = None
    
    # Configuration only (not execution state)
    self.execution_strategy: str = execution_strategy
    self.max_workers: int = max_workers if execution_strategy == "concurrent" else 1
    
    # Validate execution_timeout
    if execution_timeout is not None:
        if not isinstance(execution_timeout, (int, float)):
            raise TypeError(
                f"execution_timeout must be numeric, got {type(execution_timeout).__name__}"
            )
        if execution_timeout <= 0:
            raise ValueError(
                f"execution_timeout must be positive, got {execution_timeout}"
            )
    
    self.execution_timeout: float | None = (
        execution_timeout if execution_timeout is not None else 300.0
    )
    
    # âŒ REMOVED: All execution state fields
    # self._task_queue = queue.Queue()  # REMOVED
    # self._pending_tasks: list[SlotActivationTask] = []  # REMOVED
    # self._execution_thread: threading.Thread | None = None  # REMOVED
    # self._execution_lock: threading.Lock = threading.Lock()  # REMOVED
    # self._running: bool = False  # REMOVED
    # self._executor: ThreadPoolExecutor | None = None  # REMOVED
    # self._active_tasks: set[Future] = set()  # REMOVED
    # self._paused: bool = False  # REMOVED
    
    # Serializable fields (only structure, no execution state)
    self.add_serializable_fields(
        [
            "flow_id",
            "execution_strategy",
            "max_workers",
            "execution_timeout",
            "error_handler",
            "routines",
            "connections",
        ]
    )
    
    self._event_slot_connections: dict[tuple, Connection] = {}
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æ‰§è¡ŒçŠ¶æ€å­—æ®µå·²ç§»é™¤
- [ ] åºåˆ—åŒ–å­—æ®µåˆ—è¡¨å·²æ›´æ–°
- [ ] Flowå®Œå…¨é™æ€

---

#### Step 4.2: ä¿®æ”¹Flow.start()æ–¹æ³•

**æ–‡ä»¶**: `routilux/flow/flow.py`

**ä»»åŠ¡**:
1. ä¿®æ”¹`start()`æ–¹æ³•ä½¿ç”¨GlobalJobManager
2. ç¡®ä¿æ–¹æ³•ç­¾åæ­£ç¡®

**å…·ä½“å®ç°è¦æ±‚**:

```python
def start(
    self,
    entry_routine_id: str,
    entry_params: dict[str, Any] | None = None,
    timeout: float | None = None,
    job_state: JobState | None = None,
) -> JobState:
    """Start flow execution asynchronously.
    
    This method starts execution and returns immediately with a JobState.
    Execution continues in background using GlobalJobManager.
    
    Args:
        entry_routine_id: Entry routine identifier.
        entry_params: Entry parameters.
        timeout: Execution timeout.
        job_state: Optional existing JobState.
    
    Returns:
        JobState (status will be RUNNING initially).
    
    Raises:
        ValueError: If entry_routine_id not found.
    """
    from routilux.job_manager import get_job_manager
    
    job_manager = get_job_manager()
    return job_manager.start_job(
        flow=self,
        entry_routine_id=entry_routine_id,
        entry_params=entry_params,
        timeout=timeout,
        job_state=job_state,
    )
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] start()æ–¹æ³•æ­£ç¡®ä½¿ç”¨GlobalJobManager
- [ ] ç«‹å³è¿”å›JobState
- [ ] æ‰§è¡Œåœ¨åå°è¿›è¡Œ

---

#### Step 4.3: ä¿®æ”¹Flow.pause()ã€resume()ã€cancel()æ–¹æ³•

**æ–‡ä»¶**: `routilux/flow/flow.py`

**ä»»åŠ¡**:
1. ä¿®æ”¹è¿™äº›æ–¹æ³•ä½¿ç”¨JobExecutor
2. é€šè¿‡JobStateæ‰¾åˆ°JobExecutor

**å…·ä½“å®ç°è¦æ±‚**:

```python
def pause(self, job_state: JobState, reason: str = "", checkpoint: dict[str, Any] | None = None) -> None:
    """Pause job execution.
    
    Args:
        job_state: JobState to pause.
        reason: Reason for pausing.
        checkpoint: Optional checkpoint data.
    
    Raises:
        ValueError: If job_state flow_id doesn't match.
    """
    if job_state.flow_id != self.flow_id:
        raise ValueError(
            f"JobState flow_id '{job_state.flow_id}' does not match Flow flow_id '{self.flow_id}'"
        )
    
    from routilux.job_manager import get_job_manager
    job_manager = get_job_manager()
    executor = job_manager.get_job(job_state.job_id)
    
    if executor is None:
        raise ValueError(f"Job {job_state.job_id} is not running")
    
    executor.pause(reason=reason, checkpoint=checkpoint)


def resume(self, job_state: JobState) -> JobState:
    """Resume job execution.
    
    Args:
        job_state: JobState to resume.
    
    Returns:
        Updated JobState.
    
    Raises:
        ValueError: If job_state flow_id doesn't match or job not found.
    """
    if job_state.flow_id != self.flow_id:
        raise ValueError(
            f"JobState flow_id '{job_state.flow_id}' does not match Flow flow_id '{self.flow_id}'"
        )
    
    from routilux.job_manager import get_job_manager
    job_manager = get_job_manager()
    executor = job_manager.get_job(job_state.job_id)
    
    if executor is None:
        # Job not running, start it
        return job_manager.start_job(
            flow=self,
            entry_routine_id=job_state.current_routine_id or "",
            job_state=job_state,
        )
    
    return executor.resume()


def cancel(self, job_state: JobState, reason: str = "") -> None:
    """Cancel job execution.
    
    Args:
        job_state: JobState to cancel.
        reason: Reason for cancellation.
    
    Raises:
        ValueError: If job_state flow_id doesn't match.
    """
    if job_state.flow_id != self.flow_id:
        raise ValueError(
            f"JobState flow_id '{job_state.flow_id}' does not match Flow flow_id '{self.flow_id}'"
        )
    
    from routilux.job_manager import get_job_manager
    job_manager = get_job_manager()
    executor = job_manager.get_job(job_state.job_id)
    
    if executor is None:
        # Job not running, just mark as cancelled
        from routilux.status import ExecutionStatus
        job_state.status = ExecutionStatus.CANCELLED
        return
    
    executor.cancel(reason=reason)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] pause()æ­£ç¡®å·¥ä½œ
- [ ] resume()æ­£ç¡®å·¥ä½œ
- [ ] cancel()æ­£ç¡®å·¥ä½œ

---

#### Step 4.4: ç§»é™¤Flowä¸­ä¸å†éœ€è¦çš„æ–¹æ³•

**æ–‡ä»¶**: `routilux/flow/flow.py`

**ä»»åŠ¡**:
1. ç§»é™¤`_get_executor()`æ–¹æ³•
2. ç§»é™¤`set_execution_strategy()`ä¸­çš„executoråˆ›å»ºé€»è¾‘
3. ç§»é™¤`wait_for_completion()`æ–¹æ³•ï¼ˆå·²deprecatedï¼‰
4. ç§»é™¤`shutdown()`æ–¹æ³•ï¼ˆä¸å†éœ€è¦ï¼‰

**å…·ä½“å®ç°è¦æ±‚**:

```python
# âŒ REMOVE: _get_executor() method
# âŒ REMOVE: set_execution_strategy() executor creation logic
# âŒ REMOVE: wait_for_completion() method (already deprecated)
# âŒ REMOVE: shutdown() method

# âœ… KEEP: set_execution_strategy() for configuration only
def set_execution_strategy(self, strategy: str, max_workers: int | None = None) -> None:
    """Set execution strategy (configuration only).
    
    Args:
        strategy: Execution strategy.
        max_workers: Max workers (configuration only).
    """
    if strategy not in ["sequential", "concurrent"]:
        raise ValueError(
            f"Invalid execution strategy: {strategy}. Must be 'sequential' or 'concurrent'"
        )
    
    self.execution_strategy = strategy
    if strategy == "sequential":
        self.max_workers = 1
    elif max_workers is not None:
        self.max_workers = max_workers
    else:
        self.max_workers = 5
    
    # âŒ REMOVED: Executor creation logic
    # Thread pool is now managed by GlobalJobManager
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰ä¸éœ€è¦çš„æ–¹æ³•å·²ç§»é™¤
- [ ] set_execution_strategy()åªç”¨äºé…ç½®

---

### Phase 5: ä¿®æ”¹æ‰§è¡Œç›¸å…³æ¨¡å—

#### Step 5.1: ä¿®æ”¹execute_sequential()å’Œexecute_concurrent()

**æ–‡ä»¶**: `routilux/flow/execution.py`

**ä»»åŠ¡**:
1. ä¿®æ”¹è¿™äº›å‡½æ•°ä½¿ç”¨JobExecutor
2. æˆ–è€…æ ‡è®°ä¸ºdeprecatedï¼ˆå¦‚æœä¸å†éœ€è¦ï¼‰

**å…·ä½“å®ç°è¦æ±‚**:

ç”±äºç”¨æˆ·è¯´ä¸éœ€è¦å‘åå…¼å®¹ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š
- **é€‰é¡¹A**: å®Œå…¨ç§»é™¤execute()ç›¸å…³å‡½æ•°ï¼ˆå¦‚æœä¸å†éœ€è¦åŒæ­¥æ‰§è¡Œï¼‰
- **é€‰é¡¹B**: ä¿ç•™ä½†é‡æ„ä¸ºä½¿ç”¨JobExecutorï¼ˆå¦‚æœéœ€è¦åŒæ­¥æ‰§è¡Œï¼‰

**å»ºè®®é‡‡ç”¨é€‰é¡¹B**ï¼Œå› ä¸ºåŒæ­¥æ‰§è¡Œåœ¨æŸäº›åœºæ™¯ä¸‹ä»ç„¶æœ‰ç”¨ï¼š

```python
def execute_sequential(
    flow: "Flow",
    entry_routine_id: str,
    entry_params: Optional[Dict[str, Any]] = None,
    timeout: Optional[float] = None,
    job_state: Optional["JobState"] = None,
) -> "JobState":
    """Execute flow synchronously (waits for completion).
    
    This method uses JobExecutor but waits for completion.
    For async execution, use flow.start() instead.
    
    Args:
        flow: Flow to execute.
        entry_routine_id: Entry routine identifier.
        entry_params: Entry parameters.
        timeout: Execution timeout.
        job_state: Optional existing JobState.
    
    Returns:
        JobState (completed or failed).
    """
    from routilux.job_manager import get_job_manager
    from routilux.job_state import JobState as JobStateClass
    
    job_manager = get_job_manager()
    
    # Start job
    if job_state is None:
        job_state = JobStateClass(flow.flow_id)
    
    started_job_state = job_manager.start_job(
        flow=flow,
        entry_routine_id=entry_routine_id,
        entry_params=entry_params,
        timeout=timeout,
        job_state=job_state,
    )
    
    # Wait for completion
    executor = job_manager.get_job(started_job_state.job_id)
    if executor:
        # Wait for event loop to complete
        if executor.event_loop_thread:
            executor.event_loop_thread.join(timeout=timeout)
    
    return started_job_state
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] execute_sequential()ä½¿ç”¨JobExecutor
- [ ] åŒæ­¥ç­‰å¾…å®Œæˆ
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡

---

#### Step 5.2: ä¿®æ”¹error_handling.py

**æ–‡ä»¶**: `routilux/flow/error_handling.py`

**ä»»åŠ¡**:
1. ä¿®æ”¹`handle_task_error()`å‡½æ•°ï¼Œä½¿å…¶è·¯ç”±retryä»»åŠ¡åˆ°JobExecutor
2. ç§»é™¤å¯¹`flow._running`çš„å¼•ç”¨ï¼ˆæ”¹ä¸ºJobExecutorï¼‰
3. ç¡®ä¿é”™è¯¯å¤„ç†æ­£ç¡®æ›´æ–°JobState

**å…·ä½“å®ç°è¦æ±‚**:

```python
def handle_task_error(
    task: "SlotActivationTask",
    error: Exception,
    flow: "Flow",
) -> None:
    """Handle task execution error.
    
    Modified to route retry tasks to JobExecutor instead of Flow.
    """
    # ... existing error handling logic ...
    
    if error_handler:
        should_retry = error_handler.handle_error(
            error, routine, routine_id, flow, job_state=task.job_state
        )
        
        if error_handler.strategy.value == "retry":
            if should_retry:
                max_retries = (
                    error_handler.max_retries if error_handler.max_retries > 0 else task.max_retries
                )
                if task.retry_count < max_retries:
                    from routilux.flow.task import SlotActivationTask
                    
                    retry_task = SlotActivationTask(
                        slot=task.slot,
                        data=task.data,
                        connection=task.connection,
                        priority=task.priority,
                        retry_count=task.retry_count + 1,
                        max_retries=max_retries,
                        job_state=task.job_state,
                    )
                    
                    # Route to JobExecutor instead of flow._enqueue_task()
                    if task.job_state:
                        from routilux.job_manager import get_job_manager
                        job_manager = get_job_manager()
                        executor = job_manager.get_job(task.job_state.job_id)
                        if executor:
                            executor.enqueue_task(retry_task)
                            return
                    
                    # Fallback: if no executor found, mark job as failed
                    if task.job_state:
                        from routilux.status import ExecutionStatus
                        task.job_state.status = ExecutionStatus.FAILED
                    return
        
        # ... other error handling strategies ...
    
    # Update JobState on failure
    if task.job_state:
        from routilux.status import ExecutionStatus
        task.job_state.status = ExecutionStatus.FAILED
        if routine_id:
            task.job_state.update_routine_state(
                routine_id, {"status": "failed", "error": str(error)}
            )
    
    # Stop JobExecutor instead of flow._running
    if task.job_state:
        from routilux.job_manager import get_job_manager
        job_manager = get_job_manager()
        executor = job_manager.get_job(task.job_state.job_id)
        if executor:
            executor._running = False
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] retryä»»åŠ¡æ­£ç¡®è·¯ç”±åˆ°JobExecutor
- [ ] é”™è¯¯å¤„ç†æ­£ç¡®æ›´æ–°JobState
- [ ] ä¸å†å¼•ç”¨flow._running

---

#### Step 5.3: ç§»é™¤æˆ–ä¿®æ”¹event_loop.py

**æ–‡ä»¶**: `routilux/flow/event_loop.py`

**ä»»åŠ¡**:
1. ç§»é™¤`start_event_loop()`å‡½æ•°ï¼ˆä¸å†éœ€è¦ï¼‰
2. ç§»é™¤`event_loop()`å‡½æ•°ï¼ˆç§»åˆ°JobExecutorä¸­ï¼‰
3. ä¿ç•™`enqueue_task()`ç”¨äºlegacyå…¼å®¹ï¼ˆæ ‡è®°ä¸ºdeprecatedï¼‰
4. ä¿ç•™`execute_task()`å‡½æ•°ï¼ˆJobExecutorä¼šä½¿ç”¨ï¼‰

**å…·ä½“å®ç°è¦æ±‚**:

```python
# âŒ REMOVE: start_event_loop() function
# âŒ REMOVE: event_loop() function (moved to JobExecutor)

# âœ… KEEP: execute_task() for JobExecutor to use
def execute_task(task: "SlotActivationTask", flow: "Flow") -> None:
    """Execute a single task (used by JobExecutor).
    
    Args:
        task: SlotActivationTask to execute.
        flow: Flow object.
    """
    # Implementation remains the same
    # But this is now called by JobExecutor._execute_task()
    pass


# âš ï¸ DEPRECATED: enqueue_task() for legacy compatibility
def enqueue_task(task: "SlotActivationTask", flow: "Flow") -> None:
    """Enqueue task (DEPRECATED - use JobExecutor.enqueue_task instead).
    
    This function is kept for legacy compatibility only.
    """
    import warnings
    warnings.warn(
        "enqueue_task() is deprecated. Use JobExecutor.enqueue_task() instead.",
        DeprecationWarning,
        stacklevel=2
    )
    
    # Legacy implementation
    if flow._paused:
        flow._pending_tasks.append(task)
    else:
        flow._task_queue.put(task)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ä¸éœ€è¦çš„å‡½æ•°å·²ç§»é™¤
- [ ] execute_task()ä¿ç•™
- [ ] enqueue_task()æ ‡è®°ä¸ºdeprecated

---

#### Step 5.4: ç§»é™¤æˆ–ä¿®æ”¹completion.py

**æ–‡ä»¶**: `routilux/flow/completion.py`

**ä»»åŠ¡**:
1. ç§»é™¤`ensure_event_loop_running()`ï¼ˆä¸å†éœ€è¦ï¼‰
2. ç§»é™¤`wait_for_event_loop_completion()`ï¼ˆç§»åˆ°JobExecutorä¸­ï¼‰
3. æˆ–è€…ä¿ç•™ä½†æ ‡è®°ä¸ºdeprecated

**å…·ä½“å®ç°è¦æ±‚**:

```python
# âŒ REMOVE: ensure_event_loop_running() function
# âŒ REMOVE: wait_for_event_loop_completion() function

# These functions are no longer needed because:
# - Event loop is managed by JobExecutor
# - Completion is detected by JobExecutor._is_complete()
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ä¸éœ€è¦çš„å‡½æ•°å·²ç§»é™¤

---

### Phase 6: å¤„ç†ExecutionTracker

#### Step 6.1: ç§»é™¤Flowä¸­çš„ExecutionTracker

**æ–‡ä»¶**: `routilux/flow/flow.py`

**ä»»åŠ¡**:
1. ä»Flow.__init__()ä¸­ç§»é™¤`self.execution_tracker`
2. ExecutionTrackerç°åœ¨åœ¨JobExecutorä¸­ç®¡ç†

**å…·ä½“å®ç°è¦æ±‚**:

```python
def __init__(self, ...):
    # ... existing code ...
    
    # âŒ REMOVED: ExecutionTracker from Flow
    # self.execution_tracker: ExecutionTracker | None = None  # REMOVED
    
    # ExecutionTracker is now managed by JobExecutor (one per job)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Flowä¸­ä¸å†æœ‰execution_trackerå­—æ®µ
- [ ] æ‰€æœ‰å¯¹flow.execution_trackerçš„å¼•ç”¨å·²æ›´æ–°

---

#### Step 6.2: æ›´æ–°execute_sequential()ä¸­çš„ExecutionTrackerå¼•ç”¨

**æ–‡ä»¶**: `routilux/flow/execution.py`

**ä»»åŠ¡**:
1. ç§»é™¤`flow.execution_tracker`çš„åˆ›å»ºå’Œä½¿ç”¨
2. ExecutionTrackeråœ¨JobExecutorä¸­ç®¡ç†

**å…·ä½“å®ç°è¦æ±‚**:

```python
def execute_sequential(...):
    # ... existing code ...
    
    # âŒ REMOVED: ExecutionTracker creation
    # flow.execution_tracker = ExecutionTracker(flow.flow_id)  # REMOVED
    
    # ExecutionTracker is now created in JobExecutor.start()
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ä¸å†åœ¨execute_sequential()ä¸­åˆ›å»ºExecutionTracker
- [ ] æ‰€æœ‰ExecutionTrackeræ“ä½œåœ¨JobExecutorä¸­

---

### Phase 7: æ›´æ–°APIè·¯ç”±

#### Step 6.1: æ›´æ–°jobs.py APIè·¯ç”±

**æ–‡ä»¶**: `routilux/api/routes/jobs.py`

**ä»»åŠ¡**:
1. æ›´æ–°`start_job()`ä½¿ç”¨æ–°çš„æ¶æ„
2. æ›´æ–°`pause_job()`, `resume_job()`, `cancel_job()`ä½¿ç”¨JobExecutor

**å…·ä½“å®ç°è¦æ±‚**:

```python
@router.post("/jobs", response_model=JobResponse, status_code=201)
async def start_job(request: JobStartRequest):
    """Start a new job from a flow."""
    flow = flow_store.get(request.flow_id)
    if not flow:
        raise HTTPException(status_code=404, detail=f"Flow '{request.flow_id}' not found")
    
    MonitoringRegistry.enable()
    
    # Use flow.start() which uses GlobalJobManager
    try:
        job_state = flow.start(
            entry_routine_id=request.entry_routine_id,
            entry_params=request.entry_params,
            timeout=request.timeout,
        )
        
        job_store.add(job_state)
        return _job_to_response(job_state)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to start job: {str(e)}") from e


@router.post("/jobs/{job_id}/pause", status_code=200)
async def pause_job(job_id: str):
    """Pause job execution."""
    job_state = job_store.get(job_id)
    if not job_state:
        raise HTTPException(status_code=404, detail=f"Job '{job_id}' not found")
    
    flow = flow_store.get(job_state.flow_id)
    if not flow:
        raise HTTPException(status_code=404, detail=f"Flow '{job_state.flow_id}' not found")
    
    try:
        flow.pause(job_state, reason="Paused via API")
        return {"status": "paused", "job_id": job_id}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to pause job: {str(e)}") from e


@router.post("/jobs/{job_id}/resume", status_code=200)
async def resume_job(job_id: str):
    """Resume job execution."""
    job_state = job_store.get(job_id)
    if not job_state:
        raise HTTPException(status_code=404, detail=f"Job '{job_id}' not found")
    
    flow = flow_store.get(job_state.flow_id)
    if not flow:
        raise HTTPException(status_code=404, detail=f"Flow '{job_state.flow_id}' not found")
    
    try:
        updated_job_state = flow.resume(job_state)
        job_store.add(updated_job_state)
        return {"status": "resumed", "job_id": job_id}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to resume job: {str(e)}") from e


@router.post("/jobs/{job_id}/cancel", status_code=200)
async def cancel_job(job_id: str):
    """Cancel job execution."""
    job_state = job_store.get(job_id)
    if not job_state:
        raise HTTPException(status_code=404, detail=f"Job '{job_id}' not found")
    
    flow = flow_store.get(job_state.flow_id)
    if not flow:
        raise HTTPException(status_code=404, detail=f"Flow '{job_state.flow_id}' not found")
    
    try:
        flow.cancel(job_state, reason="Cancelled via API")
        return {"status": "cancelled", "job_id": job_id}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to cancel job: {str(e)}") from e
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] APIè·¯ç”±æ­£ç¡®ä½¿ç”¨æ–°æ¶æ„
- [ ] æ‰€æœ‰APIç«¯ç‚¹æ­£å¸¸å·¥ä½œ

---

### Phase 8: æµ‹è¯•å’ŒéªŒè¯

#### Step 7.1: åˆ›å»ºå•å…ƒæµ‹è¯•

**æ–‡ä»¶**: `tests/test_job_manager.py` (æ–°å»º)

**ä»»åŠ¡**:
1. æµ‹è¯•GlobalJobManagerå•ä¾‹æ¨¡å¼
2. æµ‹è¯•jobå¯åŠ¨å’ŒæŸ¥è¯¢
3. æµ‹è¯•wait_for_all_jobs()
4. æµ‹è¯•shutdown()

**å…·ä½“æµ‹è¯•è¦æ±‚**:

```python
"""Tests for GlobalJobManager."""

import pytest
from routilux import Flow, Routine
from routilux.job_manager import get_job_manager, GlobalJobManager
from routilux.status import ExecutionStatus


class TestRoutine(Routine):
    def __init__(self):
        super().__init__()
        self.trigger_slot = self.define_slot("trigger", handler=self.handle)
        self.output_event = self.define_event("output", ["data"])
    
    def handle(self, **kwargs):
        self.emit("output", data="test")


def test_global_job_manager_singleton():
    """Test that GlobalJobManager is a singleton."""
    manager1 = get_job_manager(max_workers=50)
    manager2 = get_job_manager(max_workers=100)  # Should return same instance
    
    assert manager1 is manager2
    assert manager1.max_workers == 50  # First call's value


def test_start_job():
    """Test starting a job."""
    flow = Flow(flow_id="test_flow")
    routine = TestRoutine()
    flow.add_routine(routine, "test")
    
    manager = get_job_manager()
    job_state = manager.start_job(
        flow=flow,
        entry_routine_id="test",
        entry_params={"data": "test"}
    )
    
    assert job_state.status == ExecutionStatus.RUNNING
    assert job_state.job_id is not None
    
    # Wait for completion
    import time
    time.sleep(0.5)
    
    executor = manager.get_job(job_state.job_id)
    assert executor is not None


def test_wait_for_all_jobs():
    """Test waiting for all jobs."""
    flow = Flow(flow_id="test_flow")
    routine = TestRoutine()
    flow.add_routine(routine, "test")
    
    manager = get_job_manager()
    
    # Start multiple jobs
    job1 = manager.start_job(flow, "test")
    job2 = manager.start_job(flow, "test")
    job3 = manager.start_job(flow, "test")
    
    # Wait for all
    completed = manager.wait_for_all_jobs(timeout=5.0)
    assert completed is True
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] æµ‹è¯•è¦†ç›–ç‡ > 80%

---

#### Step 7.2: åˆ›å»ºé›†æˆæµ‹è¯•

**æ–‡ä»¶**: `tests/test_job_executor.py` (æ–°å»º)

**ä»»åŠ¡**:
1. æµ‹è¯•JobExecutoråŸºæœ¬åŠŸèƒ½
2. æµ‹è¯•æš‚åœ/æ¢å¤/å–æ¶ˆ
3. æµ‹è¯•è¶…æ—¶å¤„ç†
4. æµ‹è¯•å¤šjobå¹¶å‘æ‰§è¡Œ

**å…·ä½“æµ‹è¯•è¦æ±‚**:

```python
"""Tests for JobExecutor."""

import pytest
import time
from routilux import Flow, Routine
from routilux.job_executor import JobExecutor
from routilux.job_manager import get_job_manager
from routilux.job_state import JobState
from routilux.status import ExecutionStatus


def test_job_executor_basic():
    """Test basic JobExecutor functionality."""
    # Create flow and job
    flow = Flow(flow_id="test")
    routine = TestRoutine()
    flow.add_routine(routine, "test")
    
    job_state = JobState(flow.flow_id)
    manager = get_job_manager()
    
    executor = JobExecutor(
        flow=flow,
        job_state=job_state,
        global_thread_pool=manager.global_thread_pool,
    )
    
    executor.start("test", {})
    
    # Wait a bit
    time.sleep(0.5)
    
    # Check status
    assert job_state.status in [ExecutionStatus.RUNNING, ExecutionStatus.COMPLETED]


def test_job_executor_pause_resume():
    """Test pause and resume."""
    # Similar structure
    pass


def test_job_executor_timeout():
    """Test timeout handling."""
    # Similar structure
    pass


def test_multiple_jobs_concurrent():
    """Test multiple jobs running concurrently."""
    flow = Flow(flow_id="test")
    routine = TestRoutine()
    flow.add_routine(routine, "test")
    
    manager = get_job_manager()
    
    # Start 10 jobs
    jobs = []
    for i in range(10):
        job = manager.start_job(flow, "test", entry_params={"index": i})
        jobs.append(job)
    
    # Wait for all
    completed = manager.wait_for_all_jobs(timeout=10.0)
    assert completed is True
    
    # Check all completed
    for job in jobs:
        executor = manager.get_job(job.job_id)
        assert executor is None or not executor.is_running()
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡
- [ ] å¤šjobå¹¶å‘æµ‹è¯•é€šè¿‡

---

#### Step 7.3: æ›´æ–°ç°æœ‰æµ‹è¯•

**ä»»åŠ¡**:
1. æ›´æ–°æ‰€æœ‰ä½¿ç”¨`flow.execute()`çš„æµ‹è¯•
2. æ›´æ–°æ‰€æœ‰ä½¿ç”¨Flowæ‰§è¡ŒçŠ¶æ€çš„æµ‹è¯•
3. ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡

**å…·ä½“è¦æ±‚**:
- æŸ¥æ‰¾æ‰€æœ‰æµ‹è¯•æ–‡ä»¶
- æ›´æ–°æµ‹è¯•ä»¥ä½¿ç”¨æ–°æ¶æ„
- è¿è¡Œæ‰€æœ‰æµ‹è¯•ç¡®ä¿é€šè¿‡

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰ç°æœ‰æµ‹è¯•æ›´æ–°
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡

---

### Phase 9: æ–‡æ¡£æ›´æ–°

#### Step 8.1: æ›´æ–°ç”¨æˆ·æ–‡æ¡£

**æ–‡ä»¶**: `docs/source/user_guide/flows.rst`

**ä»»åŠ¡**:
1. æ›´æ–°Flowæ‰§è¡Œç›¸å…³æ–‡æ¡£
2. æ·»åŠ GlobalJobManagerä½¿ç”¨è¯´æ˜
3. æ›´æ–°ç¤ºä¾‹ä»£ç 

**å…·ä½“è¦æ±‚**:
- æ›´æ–°"Working with Flows"ç« èŠ‚
- æ·»åŠ "Job Management"ç« èŠ‚
- æ›´æ–°æ‰€æœ‰ç¤ºä¾‹ä»£ç 

---

#### Step 8.2: æ›´æ–°APIæ–‡æ¡£

**æ–‡ä»¶**: `docs/source/api_reference/`

**ä»»åŠ¡**:
1. æ›´æ–°Flow APIæ–‡æ¡£
2. æ·»åŠ GlobalJobManager APIæ–‡æ¡£
3. æ·»åŠ JobExecutor APIæ–‡æ¡£ï¼ˆå¦‚æœéœ€è¦ï¼‰

---

## å…³é”®é—æ¼ç‚¹è¡¥å……

### ğŸ”´ å‘ç°çš„å…³é”®é—æ¼

1. **ExecutionTrackerå¤„ç†**ï¼š
   - å½“å‰ExecutionTrackeråœ¨Flowçº§åˆ«
   - éœ€è¦ç§»åˆ°JobExecutorçº§åˆ«ï¼ˆæ¯ä¸ªjobä¸€ä¸ªï¼‰
   - æˆ–è€…ä¿æŒFlowçº§åˆ«ä½†ç¡®ä¿çº¿ç¨‹å®‰å…¨

2. **ä»»åŠ¡åºåˆ—åŒ–ç»†èŠ‚**ï¼š
   - éœ€è¦å®ç°å®Œæ•´çš„åºåˆ—åŒ–/ååºåˆ—åŒ–é€»è¾‘
   - éœ€è¦å¤„ç†routine_idæ˜ å°„ï¼ˆä»routine._idåˆ°flow.routinesä¸­çš„keyï¼‰

3. **é”™è¯¯å¤„ç†ä¸­çš„retryè·¯ç”±**ï¼š
   - handle_task_error()ä¸­çš„retryä»»åŠ¡éœ€è¦è·¯ç”±åˆ°æ­£ç¡®çš„JobExecutor
   - ä¸èƒ½ä½¿ç”¨flow._enqueue_task()

4. **Monitoring hooksè°ƒç”¨æ—¶æœº**ï¼š
   - ç¡®ä¿æ‰€æœ‰hooksåœ¨JobExecutorä¸­æ­£ç¡®è°ƒç”¨
   - on_flow_start, on_flow_end, on_slot_callç­‰

### è¡¥å……å®ç°ç»†èŠ‚

#### ExecutionTrackerå¤„ç†

**å†³ç­–**ï¼šExecutionTrackeråº”è¯¥ç§»åˆ°JobExecutorçº§åˆ«ï¼Œå› ä¸ºï¼š
- æ¯ä¸ªjobæœ‰ç‹¬ç«‹çš„æ‰§è¡Œå†å²
- é¿å…å¤šjobä¹‹é—´çš„æ•°æ®æ··ä¹±
- æ›´ç¬¦åˆjobçº§åˆ«éš”ç¦»çš„è®¾è®¡

**å®ç°**ï¼š
```python
class JobExecutor:
    def __init__(self, ...):
        # ...
        from routilux.execution_tracker import ExecutionTracker
        self.execution_tracker = ExecutionTracker(flow.flow_id)
```

#### ä»»åŠ¡åºåˆ—åŒ–å®ç°

éœ€è¦å®Œæ•´å®ç°åºåˆ—åŒ–/ååºåˆ—åŒ–ï¼Œç‰¹åˆ«æ³¨æ„routine_idæ˜ å°„ï¼š

```python
def _serialize_pending_tasks(executor: JobExecutor):
    """Serialize pending tasks to JobState."""
    serialized_tasks = []
    for task in executor.pending_tasks:
        # Find routine_id in flow (not routine._id)
        routine_id = None
        if task.slot.routine:
            for rid, r in executor.flow.routines.items():
                if r is task.slot.routine:
                    routine_id = rid
                    break
        
        serialized = {
            "routine_id": routine_id,  # Use flow's routine_id
            "slot_name": task.slot.name,
            "data": task.data,
            # ... other fields
        }
        serialized_tasks.append(serialized)
    
    executor.job_state.pending_tasks = serialized_tasks
```

#### é”™è¯¯å¤„ç†ä¸­çš„retryè·¯ç”±

ä¿®æ”¹handle_task_error()ï¼Œä½¿å…¶è·¯ç”±åˆ°JobExecutorï¼š

```python
def handle_task_error(task, error, flow):
    # ... error handling logic ...
    
    if should_retry:
        retry_task = SlotActivationTask(...)
        
        # Route to JobExecutor instead of flow._enqueue_task()
        if task.job_state:
            from routilux.job_manager import get_job_manager
            job_manager = get_job_manager()
            executor = job_manager.get_job(task.job_state.job_id)
            if executor:
                executor.enqueue_task(retry_task)
                return
```

## å…³é”®æ³¨æ„äº‹é¡¹

### âš ï¸ å¿…é¡»ä¸¥æ ¼éµå®ˆçš„è§„åˆ™

1. **ä¸å…è®¸ä¿®æ”¹JobStateç»“æ„**ï¼šJobStateæ˜¯åºåˆ—åŒ–çš„æ ¸å¿ƒï¼Œä¸èƒ½æ”¹å˜
2. **ä¸å…è®¸ä¿®æ”¹Routineæ¥å£**ï¼šRoutineæ¥å£å¿…é¡»ä¿æŒä¸å˜
3. **emit()å¿…é¡»å‘åå…¼å®¹**ï¼šå¦‚æœæ²¡æœ‰job_stateï¼Œå¿…é¡»ä½¿ç”¨legacyæ¨¡å¼
4. **çº¿ç¨‹å®‰å…¨**ï¼šæ‰€æœ‰å…±äº«èµ„æºè®¿é—®å¿…é¡»åŠ é”
5. **èµ„æºæ¸…ç†**ï¼šjobå®Œæˆåå¿…é¡»ä»GlobalJobManagerä¸­ç§»é™¤
6. **ExecutionTracker**ï¼šæ¯ä¸ªJobExecutoræœ‰ç‹¬ç«‹çš„ExecutionTracker
7. **ä»»åŠ¡åºåˆ—åŒ–**ï¼šå¿…é¡»ä½¿ç”¨flow.routinesä¸­çš„routine_idï¼Œä¸æ˜¯routine._id

### ğŸ”´ å…³é”®å®ç°ç»†èŠ‚

1. **emit()è·¯ç”±é€»è¾‘**ï¼š
   ```python
   if job_state in context:
       route to JobExecutor
   else:
       use legacy mode (direct call)
   ```

2. **JobExecutoræ¸…ç†**ï¼š
   ```python
   def _cleanup(self):
       self._running = False
       # Remove from manager
       job_manager.running_jobs.pop(self.job_state.job_id, None)
   ```

3. **è¶…æ—¶å¤„ç†**ï¼š
   ```python
   if timeout and elapsed >= timeout:
       self._handle_timeout()
       break
   ```

4. **å®Œæˆæ£€æµ‹**ï¼š
   ```python
   def _is_complete(self):
       return queue.empty() and len(active_tasks) == 0
   ```

## éªŒæ”¶æ ‡å‡†æ€»ç»“

### Phase 1-2: åŸºç¡€è®¾æ–½
- [ ] GlobalJobManageråˆ›å»ºå¹¶æµ‹è¯•é€šè¿‡
- [ ] JobExecutoråˆ›å»ºå¹¶æµ‹è¯•é€šè¿‡
- [ ] emit()è·¯ç”±æ­£ç¡®
- [ ] ExecutionTrackerç§»åˆ°JobExecutor

### Phase 3-4: åŠŸèƒ½å®ç°
- [ ] æš‚åœ/æ¢å¤/å–æ¶ˆåŠŸèƒ½æ­£ç¡®
- [ ] ä»»åŠ¡åºåˆ—åŒ–/ååºåˆ—åŒ–æ­£ç¡®
- [ ] Flowç±»æ¸…ç†å®Œæˆ
- [ ] Flow.start()æ­£ç¡®å·¥ä½œ

### Phase 5-6: é›†æˆ
- [ ] æ‰§è¡Œæ¨¡å—æ›´æ–°å®Œæˆ
- [ ] é”™è¯¯å¤„ç†æ›´æ–°å®Œæˆ
- [ ] ExecutionTrackerå¤„ç†å®Œæˆ

### Phase 7-8: APIå’Œæµ‹è¯•
- [ ] APIè·¯ç”±æ›´æ–°å®Œæˆ
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] å¤šjobå¹¶å‘æµ‹è¯•é€šè¿‡

### Phase 9: æ–‡æ¡£
- [ ] æ–‡æ¡£æ›´æ–°å®Œæˆ

## æ—¶é—´ä¼°ç®—

- Phase 1: 2-3å¤©
- Phase 2: 1-2å¤©
- Phase 3: 2-3å¤©
- Phase 4: 2-3å¤©
- Phase 5: 2-3å¤©
- Phase 6: 1å¤©
- Phase 7: 1å¤©
- Phase 8: 3-4å¤©
- Phase 9: 1-2å¤©

**æ€»è®¡**: 15-22å¤©

## é£é™©æ§åˆ¶

1. **é€æ­¥è¿ç§»**ï¼šæ¯ä¸ªPhaseå®Œæˆåè¿›è¡Œæµ‹è¯•
2. **å›æ»šè®¡åˆ’**ï¼šä¿ç•™æ—§ä»£ç ç›´åˆ°æ–°ä»£ç å®Œå…¨æµ‹è¯•é€šè¿‡
3. **ä»£ç å®¡æŸ¥**ï¼šæ¯ä¸ªPhaseå®Œæˆåè¿›è¡Œä»£ç å®¡æŸ¥
4. **é›†æˆæµ‹è¯•**ï¼šæ¯ä¸ªPhaseå®Œæˆåè¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶

## å…³é”®å®ç°ç»†èŠ‚è¡¥å……

### emit()è·¯ç”±çš„å®Œæ•´å®ç°

**å…³é”®ç‚¹**ï¼šemit()å¿…é¡»èƒ½å¤Ÿè·¯ç”±åˆ°æ­£ç¡®çš„JobExecutorï¼Œå³ä½¿åœ¨æ²¡æœ‰æ˜¾å¼ä¼ é€’çš„æƒ…å†µä¸‹ã€‚

**å®ç°é€»è¾‘**ï¼š
```python
def emit(self, flow: Flow | None = None, **kwargs):
    # 1. Auto-detect flow
    if flow is None and self.routine:
        flow = getattr(self.routine, "_current_flow", None)
    
    # 2. Get job_state from context
    from routilux.routine import _current_job_state
    job_state = _current_job_state.get(None)
    
    # 3. Route to JobExecutor if available
    if job_state is not None and flow is not None:
        from routilux.job_manager import get_job_manager
        job_manager = get_job_manager()
        executor = job_manager.get_job(job_state.job_id)
        
        if executor is not None:
            # Route to JobExecutor
            for slot in self.connected_slots:
                connection = flow._find_connection(self, slot)
                task = SlotActivationTask(
                    slot=slot,
                    data=kwargs,
                    job_state=job_state,
                    connection=connection,
                )
                executor.enqueue_task(task)
            return
    
    # 4. Legacy mode (no job_state or no executor)
    if flow is None:
        for slot in self.connected_slots:
            slot.receive(**kwargs)
        return
```

### ä»»åŠ¡åºåˆ—åŒ–çš„routine_idæ˜ å°„

**å…³é”®ç‚¹**ï¼šå¿…é¡»ä½¿ç”¨flow.routinesä¸­çš„keyä½œä¸ºroutine_idï¼Œä¸æ˜¯routine._idã€‚

**å®ç°**ï¼š
```python
# åºåˆ—åŒ–æ—¶
routine_id = None
if task.slot.routine:
    for rid, r in executor.flow.routines.items():
        if r is task.slot.routine:
            routine_id = rid  # Use flow's routine_id
            break

# ååºåˆ—åŒ–æ—¶
routine_id = serialized.get("routine_id")
if routine_id and routine_id in executor.flow.routines:
    routine = executor.flow.routines[routine_id]
    slot = routine.get_slot(slot_name)
```

### JobExecutorå®Œæˆæ£€æµ‹çš„ç¨³å®šæ€§

**å…³é”®ç‚¹**ï¼šéœ€è¦å¤šæ¬¡æ£€æŸ¥ç¡®ä¿çœŸæ­£å®Œæˆï¼Œé¿å…race conditionã€‚

**å®ç°**ï¼š
```python
def _is_complete(self) -> bool:
    """Check if job is complete (with stability check)."""
    # Check multiple times to avoid race conditions
    for _ in range(3):
        if not self.task_queue.empty():
            return False
        
        with self._lock:
            active = [f for f in self.active_tasks if not f.done()]
            if len(active) > 0:
                return False
        
        time.sleep(0.01)  # Small delay between checks
    
    return True
```

### é”™è¯¯å¤„ç†ä¸­çš„èµ„æºæ¸…ç†

**å…³é”®ç‚¹**ï¼šé”™è¯¯å‘ç”Ÿæ—¶å¿…é¡»æ­£ç¡®æ¸…ç†JobExecutorï¼Œä¸èƒ½ç•™ä¸‹åƒµå°¸jobã€‚

**å®ç°**ï¼š
```python
def _handle_error(self, error: Exception):
    """Handle job error with proper cleanup."""
    # Update job state
    self.job_state.status = ExecutionStatus.FAILED
    self.job_state.shared_data["error"] = str(error)
    
    # Stop event loop
    self._running = False
    
    # Cancel active tasks
    with self._lock:
        for future in list(self.active_tasks):
            if not future.done():
                future.cancel()
        self.active_tasks.clear()
    
    # Call hooks
    execution_hooks.on_flow_end(self.flow, self.job_state, "failed")
    
    # Cleanup will be called by _cleanup() when event loop exits
```

## æœ€ç»ˆæ£€æŸ¥æ¸…å•

### ä»£ç å®Œæ•´æ€§æ£€æŸ¥

- [ ] GlobalJobManagerå®ç°å®Œæ•´
- [ ] JobExecutorå®ç°å®Œæ•´
- [ ] emit()è·¯ç”±é€»è¾‘å®Œæ•´
- [ ] æš‚åœ/æ¢å¤/å–æ¶ˆåŠŸèƒ½å®Œæ•´
- [ ] ä»»åŠ¡åºåˆ—åŒ–/ååºåˆ—åŒ–å®Œæ•´
- [ ] é”™è¯¯å¤„ç†æ›´æ–°å®Œæ•´
- [ ] ExecutionTrackerå¤„ç†å®Œæ•´
- [ ] Flowç±»æ¸…ç†å®Œæ•´
- [ ] APIè·¯ç”±æ›´æ–°å®Œæ•´

### çº¿ç¨‹å®‰å…¨æ£€æŸ¥

- [ ] GlobalJobManagerå•ä¾‹çº¿ç¨‹å®‰å…¨
- [ ] JobExecutorçš„task_queueè®¿é—®çº¿ç¨‹å®‰å…¨
- [ ] active_tasksè®¿é—®æœ‰é”ä¿æŠ¤
- [ ] JobStateæ›´æ–°æœ‰é”ä¿æŠ¤
- [ ] running_jobså­—å…¸è®¿é—®æœ‰é”ä¿æŠ¤

### èµ„æºç®¡ç†æ£€æŸ¥

- [ ] jobå®Œæˆåä»GlobalJobManagerç§»é™¤
- [ ] event loopçº¿ç¨‹æ­£ç¡®join
- [ ] å…¨å±€çº¿ç¨‹æ± æ­£ç¡®shutdown
- [ ] æ²¡æœ‰èµ„æºæ³„æ¼

### åºåˆ—åŒ–æ£€æŸ¥

- [ ] Flowåºåˆ—åŒ–ä¸åŒ…å«æ‰§è¡ŒçŠ¶æ€
- [ ] JobStateåºåˆ—åŒ–åŒ…å«æ‰€æœ‰å¿…è¦ä¿¡æ¯
- [ ] ä»»åŠ¡åºåˆ—åŒ–ä½¿ç”¨æ­£ç¡®çš„routine_id
- [ ] ååºåˆ—åŒ–åå¯ä»¥æ­£ç¡®æ¢å¤æ‰§è¡Œ

### åŠŸèƒ½å®Œæ•´æ€§æ£€æŸ¥

- [ ] å¤šjobå¹¶å‘æ‰§è¡Œæ­£å¸¸
- [ ] æš‚åœ/æ¢å¤åŠŸèƒ½æ­£å¸¸
- [ ] å–æ¶ˆåŠŸèƒ½æ­£å¸¸
- [ ] è¶…æ—¶å¤„ç†æ­£å¸¸
- [ ] é”™è¯¯å¤„ç†æ­£å¸¸
- [ ] Monitoring hooksæ­£å¸¸è°ƒç”¨

## å¼€å‘é¡ºåºï¼ˆä¸¥æ ¼æŒ‰æ­¤é¡ºåºï¼‰

1. **Phase 1**: åˆ›å»ºåŸºç¡€è®¾æ–½ï¼ˆGlobalJobManager + JobExecutorï¼‰
2. **Phase 2**: ä¿®æ”¹emit()è·¯ç”±
3. **Phase 3**: å®ç°æš‚åœ/æ¢å¤/å–æ¶ˆ
4. **Phase 4**: æ¸…ç†Flowç±»
5. **Phase 5**: æ›´æ–°æ‰§è¡Œæ¨¡å—å’Œé”™è¯¯å¤„ç†
6. **Phase 6**: å¤„ç†ExecutionTracker
7. **Phase 7**: æ›´æ–°APIè·¯ç”±
8. **Phase 8**: æµ‹è¯•å’ŒéªŒè¯
9. **Phase 9**: æ–‡æ¡£æ›´æ–°

**âš ï¸ é‡è¦**ï¼šæ¯ä¸ªPhaseå¿…é¡»å®Œå…¨å®Œæˆå¹¶é€šè¿‡æµ‹è¯•åæ‰èƒ½è¿›å…¥ä¸‹ä¸€ä¸ªPhaseã€‚

## æœ€ç»ˆå®¡æŸ¥æ€»ç»“

### âœ… è®¾è®¡æ–¹æ¡ˆå®Œæ•´æ€§

ç»è¿‡ä¸¥æ ¼å®¡æŸ¥ï¼Œè®¾è®¡æ–¹æ¡ˆ**å®Œæ•´ä¸”æ­£ç¡®**ï¼š

1. âœ… **æ¶æ„è®¾è®¡**ï¼šJobçº§åˆ«éš”ç¦»ã€å…¨å±€çº¿ç¨‹æ± ã€åºåˆ—åŒ–å‹å¥½
2. âœ… **å…³é”®åŠŸèƒ½**ï¼šemit()è·¯ç”±ã€æš‚åœ/æ¢å¤/å–æ¶ˆã€è¶…æ—¶å¤„ç†ã€é”™è¯¯å¤„ç†
3. âœ… **å®ç°ç»†èŠ‚**ï¼šExecutionTrackerã€ä»»åŠ¡åºåˆ—åŒ–ã€ç›‘æ§hooksã€èµ„æºæ¸…ç†
4. âœ… **æµ‹è¯•è®¡åˆ’**ï¼šå•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€å¤šjobå¹¶å‘æµ‹è¯•

### ğŸ”´ å¿…é¡»ä¸¥æ ¼éµå®ˆçš„å®ç°ç»†èŠ‚

1. **emit()è·¯ç”±**ï¼šå¿…é¡»é€šè¿‡JobStateæ‰¾åˆ°JobExecutorï¼Œä¸èƒ½ä½¿ç”¨flow._enqueue_task()
2. **ä»»åŠ¡åºåˆ—åŒ–**ï¼šå¿…é¡»ä½¿ç”¨flow.routinesä¸­çš„routine_idï¼ˆkeyï¼‰ï¼Œä¸æ˜¯routine._id
3. **é”™è¯¯å¤„ç†retry**ï¼šretryä»»åŠ¡å¿…é¡»è·¯ç”±åˆ°JobExecutorï¼Œä¸èƒ½ä½¿ç”¨flow._enqueue_task()
4. **èµ„æºæ¸…ç†**ï¼šjobå®Œæˆåå¿…é¡»ä»GlobalJobManagerä¸­ç§»é™¤
5. **ExecutionTracker**ï¼šæ¯ä¸ªJobExecutoræœ‰ç‹¬ç«‹çš„ExecutionTracker
6. **Monitoring hooks**ï¼šslot.receive()ä¸­å·²è°ƒç”¨ï¼Œä¸éœ€è¦åœ¨JobExecutorä¸­é‡å¤è°ƒç”¨

### ğŸ“‹ å¼€å‘æ£€æŸ¥ç‚¹

æ¯ä¸ªPhaseå®Œæˆåå¿…é¡»æ£€æŸ¥ï¼š

1. **ä»£ç å®Œæ•´æ€§**ï¼šæ‰€æœ‰åŠŸèƒ½å®ç°å®Œæ•´
2. **çº¿ç¨‹å®‰å…¨**ï¼šæ‰€æœ‰å…±äº«èµ„æºæœ‰é”ä¿æŠ¤
3. **èµ„æºç®¡ç†**ï¼šæ²¡æœ‰èµ„æºæ³„æ¼
4. **æµ‹è¯•é€šè¿‡**ï¼šæ‰€æœ‰ç›¸å…³æµ‹è¯•é€šè¿‡
5. **æ–‡æ¡£æ›´æ–°**ï¼šç›¸å…³æ–‡æ¡£å·²æ›´æ–°

### âš ï¸ é£é™©æç¤º

1. **emit()è·¯ç”±**ï¼šå¦‚æœè·¯ç”±å¤±è´¥ï¼Œä¼šå¯¼è‡´ä»»åŠ¡ä¸¢å¤±
2. **ä»»åŠ¡åºåˆ—åŒ–**ï¼šroutine_idæ˜ å°„é”™è¯¯ä¼šå¯¼è‡´æ¢å¤å¤±è´¥
3. **èµ„æºæ¸…ç†**ï¼šæ¸…ç†ä¸å½“ä¼šå¯¼è‡´èµ„æºæ³„æ¼
4. **çº¿ç¨‹å®‰å…¨**ï¼šé”ä½¿ç”¨ä¸å½“ä¼šå¯¼è‡´æ­»é”æˆ–æ•°æ®ç«äº‰

### âœ… éªŒæ”¶æ ‡å‡†

é‡æ„å®Œæˆåå¿…é¡»æ»¡è¶³ï¼š

1. âœ… ç”¨æˆ·å¯ä»¥åˆ›å»ºå¤šä¸ªjobï¼Œæ¯ä¸ªjobç‹¬ç«‹æ‰§è¡Œ
2. âœ… æ‰€æœ‰jobå…±äº«å…¨å±€çº¿ç¨‹æ± ï¼ˆå¯è®¾ç½®å¤§å°ï¼‰
3. âœ… ä¸»çº¿ç¨‹ä¸é˜»å¡ï¼Œå¯ä»¥ç»§ç»­å¤„ç†å…¶ä»–é€»è¾‘
4. âœ… å¯ä»¥waitæ‰€æœ‰jobå®Œæˆæˆ–è½®è¯¢æ£€æŸ¥çŠ¶æ€
5. âœ… JobStateçŠ¶æ€å®æ—¶æ›´æ–°
6. âœ… å¯ä»¥éšæ—¶æŸ¥è¯¢jobçŠ¶æ€å’Œæ•°æ®
7. âœ… Flowå®Œå…¨é™æ€ï¼Œå¯ä»¥åºåˆ—åŒ–
8. âœ… JobStateå¯ä»¥ç‹¬ç«‹åºåˆ—åŒ–
9. âœ… æš‚åœ/æ¢å¤/å–æ¶ˆåŠŸèƒ½æ­£å¸¸
10. âœ… è¶…æ—¶å¤„ç†æ­£å¸¸
11. âœ… é”™è¯¯å¤„ç†æ­£å¸¸
12. âœ… å¤šjobå¹¶å‘æ‰§è¡Œæ­£å¸¸

## å¼€å‘è®¡åˆ’å®Œæˆ

æœ¬å¼€å‘è®¡åˆ’å·²ç»è¿‡ä¸¥æ ¼å®¡æŸ¥ï¼ŒåŒ…å«äº†æ‰€æœ‰å¿…è¦çš„å®ç°ç»†èŠ‚ã€‚å¼€å‘å›¢é˜Ÿå¿…é¡»**ä¸¥æ ¼æŒ‰ç…§æ­¤è®¡åˆ’æ‰§è¡Œ**ï¼Œ**ä¸å…è®¸ä»»ä½•è‡ªç”±å‘æŒ¥**ã€‚

æ¯ä¸ªStepéƒ½æœ‰æ˜ç¡®çš„ï¼š
- æ–‡ä»¶è·¯å¾„
- å…·ä½“ä»»åŠ¡
- å®ç°è¦æ±‚ï¼ˆåŒ…å«ä»£ç ç¤ºä¾‹ï¼‰
- éªŒæ”¶æ ‡å‡†

å¼€å‘è¿‡ç¨‹ä¸­å¦‚æœ‰ç–‘é—®ï¼Œå¿…é¡»å‚è€ƒæœ¬è®¡åˆ’ï¼Œä¸å¾—è‡ªè¡Œå†³å®šå®ç°æ–¹å¼ã€‚
