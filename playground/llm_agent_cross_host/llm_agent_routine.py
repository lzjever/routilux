"""
LLM Agent Routine for intelligent agent workflows.

This routine demonstrates how to:
- Use LLM to generate responses and questions
- Pause execution when user input is needed
- Save state to cloud storage
- Resume execution after user input
"""

from typing import Dict, Any, Optional
from playground.llm_agent_cross_host.enhanced_routine import EnhancedRoutine
from playground.llm_agent_cross_host.mock_llm import get_llm_service
from playground.llm_agent_cross_host.logger import get_logger
from serilux import register_serializable


@register_serializable
class LLMAgentRoutine(EnhancedRoutine):
    """LLM Agent Routine that can pause execution to wait for user input.
    
    This routine demonstrates a complete LLM agent workflow:
    1. Process input using LLM
    2. Generate questions when needed
    3. Pause execution and save state
    4. Resume after user input
    """
    
    def __init__(self):
        super().__init__()
        
        # Define slots
        self.trigger_slot = self.define_slot("trigger", handler=self.process)
        self.user_input_slot = self.define_slot("user_input", handler=self.handle_user_input)
        self.continue_slot = self.define_slot("continue", handler=self.continue_processing)
        
        # Define events
        self.output_event = self.define_event("output", ["result", "status"])
        self.question_event = self.define_event("question", ["question", "context"])
        self.completed_event = self.define_event("completed", ["result"])
        
        # Configuration
        self.set_config(
            auto_save_on_pause=True,  # Automatically save state when pausing
            llm_model="mock-gpt-4",
        )
    
    def process(self, task=None, **kwargs):
        """Main processing logic.
        
        This method:
        1. Uses LLM to process the task
        2. If LLM generates a question, pauses execution
        3. Saves state to cloud storage
        
        Execution Flow:
        - Receives task via trigger slot
        - Calls LLM service to process task
        - If LLM needs user input:
          * Stores LLM response and question in shared_data
          * Emits deferred event (will be emitted on resume)
          * Saves execution state to cloud storage
          * Pauses execution
        - If no user input needed:
          * Emits output event and completes
        """
        logger = get_logger()
        ctx = self.get_execution_context()
        if not ctx:
            logger.warning("ROUTINE", "æ— æ³•è·å–æ‰§è¡Œä¸Šä¸‹æ–‡")
            return
        
        # Extract task from input
        task_data = task or kwargs.get("task", "é»˜è®¤ä»»åŠ¡")
        logger.info("ROUTINE", f"å¼€å§‹å¤„ç†ä»»åŠ¡", routine_id=ctx.routine_id, task=task_data)
        
        # Log processing start
        ctx.job_state.append_to_shared_log({
            "action": "process_start",
            "task": task_data,
            "routine_id": ctx.routine_id,
        })
        
        # Use LLM to process
        logger.debug("LLM", "è°ƒç”¨LLMæœåŠ¡å¤„ç†ä»»åŠ¡", task=task_data)
        llm_service = get_llm_service()
        
        # Simulate LLM processing
        context = f"å¤„ç†ä»»åŠ¡: {task_data}"
        llm_result = llm_service.generate_with_question(context)
        
        logger.info("LLM", "LLMå¤„ç†å®Œæˆ", 
                   response=llm_result["response"][:50] + "...",
                   needs_user_input=llm_result["needs_user_input"])
        
        # Store LLM result in shared data
        ctx.job_state.update_shared_data("llm_response", llm_result["response"])
        ctx.job_state.update_shared_data("llm_question", llm_result["question"])
        ctx.job_state.update_shared_data("task", task_data)
        logger.debug("STATE", "LLMç»“æœå·²ä¿å­˜åˆ°shared_data", 
                   keys=["llm_response", "llm_question", "task"])
        
        # Update routine state
        ctx.job_state.update_routine_state(ctx.routine_id, {
            "status": "processing",
            "llm_response": llm_result["response"],
            "needs_user_input": llm_result["needs_user_input"],
        })
        logger.debug("STATE", "RoutineçŠ¶æ€å·²æ›´æ–°", routine_id=ctx.routine_id, status="processing")
        
        # If LLM needs user input, pause execution
        if llm_result.get("needs_user_input", False):
            question = llm_result["question"]
            
            logger.info("ROUTINE", f"ğŸ¤– LLMéœ€è¦ç”¨æˆ·è¾“å…¥", question=question)
            
            # Emit deferred event (will be emitted on resume)
            self.emit_deferred_event("question", question=question, context=context)
            logger.debug("EVENT", "å»¶è¿Ÿäº‹ä»¶å·²æ·»åŠ ", event="question", 
                       will_emit_on="resume")
            
            # Save state to cloud storage (if configured)
            if self.get_config("auto_save_on_pause", default=True):
                logger.info("STORAGE", "å¼€å§‹ä¿å­˜æ‰§è¡ŒçŠ¶æ€åˆ°äº‘å­˜å‚¨...")
                storage_key = self.save_execution_state()
                ctx.job_state.update_shared_data("storage_key", storage_key)
                logger.info("STORAGE", f"ğŸ’¾ çŠ¶æ€å·²ä¿å­˜", storage_key=storage_key)
            
            # Log pause
            ctx.job_state.append_to_shared_log({
                "action": "paused",
                "reason": "ç­‰å¾…ç”¨æˆ·è¾“å…¥",
                "question": question,
            })
            
            # Pause execution (this will block until all active tasks complete)
            logger.info("EXECUTION", "â¸ï¸  å‡†å¤‡æš‚åœæ‰§è¡Œ...")
            logger.debug("EXECUTION", "æš‚åœå‰æ£€æŸ¥ç‚¹", 
                        checkpoint={
                            "question": question,
                            "context": context,
                            "task": task_data,
                            "waiting_for": "user_input",
                        })
            
            self.pause_execution(
                reason="ç­‰å¾…ç”¨æˆ·è¾“å…¥",
                checkpoint={
                    "question": question,
                    "context": context,
                    "task": task_data,
                    "waiting_for": "user_input",
                }
            )
            
            logger.info("EXECUTION", "âœ… æ‰§è¡Œå·²æš‚åœ", 
                       job_id=ctx.job_state.job_id,
                       status=ctx.job_state.status)
        else:
            # No user input needed, continue processing
            logger.info("ROUTINE", "æ— éœ€ç”¨æˆ·è¾“å…¥ï¼Œç»§ç»­å¤„ç†")
            result = f"å¤„ç†å®Œæˆ: {llm_result['response']}"
            self.emit("output", result=result, status="completed")
            logger.debug("EVENT", "è¾“å‡ºäº‹ä»¶å·²å‘é€", event="output", result=result[:50])
            
            ctx.job_state.update_routine_state(ctx.routine_id, {
                "status": "completed",
                "result": result,
            })
            logger.info("ROUTINE", "å¤„ç†å®Œæˆ", routine_id=ctx.routine_id, status="completed")
    
    def handle_user_input(self, user_response=None, **kwargs):
        """Handle user input after resume.
        
        This method is called when the execution is resumed and user input
        is available.
        
        Execution Flow:
        - Receives user response via user_input slot
        - Retrieves previous context (question, task) from shared_data
        - Calls LLM to process user response
        - Updates shared_data with user response and final result
        - Emits output and completed events
        - Updates routine state to completed
        """
        logger = get_logger()
        ctx = self.get_execution_context()
        if not ctx:
            logger.warning("ROUTINE", "æ— æ³•è·å–æ‰§è¡Œä¸Šä¸‹æ–‡")
            return
        
        # Extract user response
        response = user_response or kwargs.get("user_response", "")
        logger.info("ROUTINE", "ğŸ‘¤ æ”¶åˆ°ç”¨æˆ·è¾“å…¥", 
                   routine_id=ctx.routine_id,
                   response=response)
        
        # Get previous context
        question = ctx.job_state.get_shared_data("llm_question", "")
        task = ctx.job_state.get_shared_data("task", "")
        logger.debug("STATE", "è·å–ä¸Šä¸‹æ–‡", question=question[:50], task=task)
        
        # Log user input
        ctx.job_state.append_to_shared_log({
            "action": "user_input_received",
            "response": response,
            "question": question,
        })
        
        # Process user response
        logger.info("LLM", "ä½¿ç”¨LLMå¤„ç†ç”¨æˆ·å“åº”...")
        llm_service = get_llm_service()
        prompt = f"ç”¨æˆ·å›å¤: {response}\né’ˆå¯¹é—®é¢˜: {question}\nä»»åŠ¡: {task}"
        final_result = llm_service.generate(prompt)
        logger.info("LLM", "LLMå¤„ç†å®Œæˆ", result=final_result[:50] + "...")
        
        # Update state
        ctx.job_state.update_shared_data("user_response", response)
        ctx.job_state.update_shared_data("final_result", final_result)
        logger.debug("STATE", "ç”¨æˆ·å“åº”å’Œæœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°shared_data")
        
        # Emit result
        self.emit("output", result=final_result, status="completed")
        self.emit("completed", result=final_result)
        logger.debug("EVENT", "è¾“å‡ºäº‹ä»¶å·²å‘é€", events=["output", "completed"])
        
        # Update routine state
        ctx.job_state.update_routine_state(ctx.routine_id, {
            "status": "completed",
            "user_response": response,
            "result": final_result,
        })
        logger.info("ROUTINE", "âœ… ç”¨æˆ·è¾“å…¥å¤„ç†å®Œæˆ", 
                   routine_id=ctx.routine_id,
                   status="completed")
    
    def continue_processing(self, **kwargs):
        """Continue processing after user input.
        
        This is an alternative handler that can be used to continue
        processing with additional data.
        """
        ctx = self.get_execution_context()
        if not ctx:
            return
        
        # Get previous state
        task = ctx.job_state.get_shared_data("task", "")
        user_response = ctx.job_state.get_shared_data("user_response", "")
        
        # Continue processing
        llm_service = get_llm_service()
        prompt = f"ç»§ç»­å¤„ç†ä»»åŠ¡: {task}\nç”¨æˆ·è¾“å…¥: {user_response}"
        result = llm_service.generate(prompt)
        
        # Emit result
        self.emit("output", result=result, status="continued")
        
        # Update state
        ctx.job_state.update_routine_state(ctx.routine_id, {
            "status": "continued",
            "result": result,
        })

